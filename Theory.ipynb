{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab  Report on : Data Science and Analytics"
      ],
      "metadata": {
        "id": "FgFOtJTNK9kL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data PreProcessing\n"
      ],
      "metadata": {
        "id": "W7zx6P6oLHGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing prepares raw data for further processing."
      ],
      "metadata": {
        "id": "9swp2drzLQeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps in Data Preprocessing\n",
        "- 1.Data Cleaning\n",
        "- 2.Data Integration\n",
        "- 3.Data Transformation\n",
        "- 4.Data Reduction"
      ],
      "metadata": {
        "id": "7ZC5bTQALSMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Data Cleaning\n"
      ],
      "metadata": {
        "id": "J4wG2ficLpjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is the process of identifying and correcting errors or inconsistencies in the dataset. It involves handling missing values, removing duplicates, and correcting incorrect or outlier data to ensure the dataset is accurate and reliable. Clean data is essential for effective analysis, as it improves the quality of results and enhances the performance of data models."
      ],
      "metadata": {
        "id": "nJBKyUCJL1q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Missing Values:\n",
        " This occur when data is absent from a dataset. You can either ignore the rows with missing data or fill the gaps manually, with the attribute mean, or by using the most probable value. This ensures the dataset remains accurate and complete for analysis.\n"
      ],
      "metadata": {
        "id": "wDN-0GpNL2eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Noisy Data:\n",
        "It refers to irrelevant or incorrect data that is difficult for machines to interpret, often caused by errors in data collection or entry. It can be handled in several ways:\n",
        "  - Binning Method:\n",
        "  The data is sorted into equal segments, and each segment is smoothed by replacing values with the mean or boundary values.\n",
        " - Regression:\n",
        "  Data can be smoothed by fitting it to a regression function, either linear or multiple, to predict values.\n",
        "  - Clustering:\n",
        "   This method groups similar data points together, with outliers either being undetected or falling outside the clusters. These techniques help remove noise and improve data quality.\n",
        "\n",
        "- Removing Duplicates:\n",
        " It involves identifying and eliminating repeated data entries to ensure accuracy and consistency in the dataset. This process prevents errors and ensures reliable analysis by keeping only unique records.\n"
      ],
      "metadata": {
        "id": "un6fHbMkMCCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Data Integration"
      ],
      "metadata": {
        "id": "NO9Yr_guMg7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It involves merging data from various sources into a single, unified dataset. It can be challenging due to differences in data formats, structures, and meanings. Techniques like record linkage and data fusion help in combining data efficiently, ensuring consistency and accuracy."
      ],
      "metadata": {
        "id": "CvhX3LvwMqEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Record Linkage** is the process of identifying and matching records from different datasets that refer to the same entity, even if they are represented differently. It helps in combining data from various sources by finding corresponding records based on common identifiers or attributes.\n",
        "\n",
        "**Data Fusion** involves combining data from multiple sources to create a more comprehensive and accurate dataset. It integrates information that may be inconsistent or incomplete from different sources, ensuring a unified and richer dataset for analysis.\n"
      ],
      "metadata": {
        "id": "78GV4kmpMqwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Data Transformation\n"
      ],
      "metadata": {
        "id": "zMNFk4ckMzxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It involves converting data into a format suitable for analysis. Common techniques include normalization, which scales data to a common range; standardization, which adjusts data to have zero mean and unit variance; and discretization, which converts continuous data into discrete categories. These techniques help prepare the data for more accurate analysis."
      ],
      "metadata": {
        "id": "beiDLvj5NJAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Data Normalization :**\n",
        "  The process of scaling data to a common range to ensure consistency across variables.\n",
        "\n",
        "- **Discretization** :\n",
        "  Converting continuous data into discrete categories for easier analysis.\n",
        "\n",
        "- **Data Aggregation :**\n",
        " Combining multiple data points into a summary form, such as averages or totals, to simplify analysis.\n",
        "\n",
        "- **Concept Hierarchy Generation** :\n",
        " Organizing data into a hierarchy of concepts to provide a higher-level view for better understanding and analysis.\n"
      ],
      "metadata": {
        "id": "u_zkvXM5NJTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Data Reduction"
      ],
      "metadata": {
        "id": "XbInLgOwN3ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It reduces the dataset's size while maintaining key information. This can be done through feature selection, which chooses the most relevant features, and feature extraction, which transforms the data into a lower-dimensional space while preserving important details. It uses various reduction techniques such as"
      ],
      "metadata": {
        "id": "v_H9InBON9zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Dimensionality Reduction (e.g., Principal Component Analysis):**\n",
        " A technique that reduces the number of variables in a dataset while retaining its essential information.\n",
        "\n",
        "- **Numerosity Reduction:**\n",
        " Reducing the number of data points by methods like sampling to simplify the dataset without losing critical patterns.\n",
        "\n",
        "- **Data Compression: **\n",
        "Reducing the size of data by encoding it in a more compact form, making it easier to store and process.\n"
      ],
      "metadata": {
        "id": "Ha6N9s4kOBuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OIve-vXIOXD-"
      }
    }
  ]
}